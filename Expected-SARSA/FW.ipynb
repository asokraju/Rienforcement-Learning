{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Expected SARSA",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNEPiQF6UbdUJEw3dkSTXFD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asokraju/Rienforcement-learning/blob/master/Expected-SARSA/FW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBs9H-eI7Dib",
        "colab_type": "text"
      },
      "source": [
        "# *On policy* and *Off policy* Learning\n",
        "\n",
        "\n",
        "In this document we impliment the following algorithms and distinguish between on policy and off policy implimentation.\n",
        "\n",
        "1.   SARSA : *On Policy* method; The Q(s,a) function is learned from action $a, ~a'$ that we sampled from the current policy $\\pi$. The update rule is:\n",
        "\n",
        "$$Q(s,a) \\leftarrow Q(s,a)+\\alpha (r+\\gamma Q(s',a')-Q(s,a))$$ \n",
        "\n",
        "2.   Q-Learning : *Off policy* method; $Q(s,a)$ function is learned from different actions (for example, random actions). a is sampled from $\\pi$ and $a'$ is a greedy policy.\n",
        "$$Q(s,a) \\leftarrow Q(s,a)+\\alpha (r+\\gamma~ max_{a'}Q(s',a')-Q(s,a))$$ \n",
        "*Note*: The distiction dissappers when $\\pi$ is a greedy policy. See [this](https://stats.stackexchange.com/questions/184657/what-is-the-difference-between-off-policy-and-on-policy-learning). Off policy methods do not need a policy, we can still find the optimal action-value function. However, it is common to use a *greedy* (deterministic) or $\\epsilon-$*greedy* (stocastic) policy $\\pi$. This gives rise to confusion. \n",
        "\n",
        "3. Expected SARSA: Both earlier methods have high variance because, neither $Q(s',a')$ nor $max_{a'}Q(s',a')$ are accurate furture action value. Expected SARSA marginalises out $a'$, which reduces the variance. \n",
        "$$Q(s,a) \\leftarrow Q(s,a)+\\alpha (r+\\gamma\\int_{a'} Q(s',a')d\\tilde\\pi(a'|s')-Q(s,a))$$ \n",
        "\n",
        "where $a$ is sampled from the policy$\\pi$. See [this](https://ai.stackexchange.com/questions/10798/expected-sarsa-vs-sarsa-in-rl-an-introduction#). Note that $\\tilde \\pi$ can be any random known policy. However, in the case $\\tilde \\pi = \\pi$, EXpected SARSA is On-policy methodology. Otherwise an off-policy technique.\n",
        "\n",
        "The final difference between an *on-policy* and *off-policy* is how we compute the TD error $(\\delta)$:\n",
        "$$\\delta  = (r+\\gamma Q(s',a')-Q(s,a))$$ \n",
        "where $a'$ sampled from $\\pi\\implies $ *on-policy* and $a'$ NOT sampled from $\\pi\\implies $ *off-policy*.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVc_XUf6KzIz",
        "colab_type": "text"
      },
      "source": [
        "#Q learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwnKfb-z_hZR",
        "colab_type": "code",
        "outputId": "dd94215c-f788-4832-bb43-28033488611e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(\" we are currently using Tensorflow version {}\".format(tf.__version__))\n",
        "import numpy as np\n",
        "import random\n",
        "from IPython.display import clear_output\n",
        "from collections import deque\n",
        "import progressbar\n",
        "\n",
        "import gym\n",
        "\n",
        "from tensorflow.keras import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, Reshape\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow import keras\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            " we are currently using Tensorflow version 2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUGOVuksK7UZ",
        "colab_type": "code",
        "outputId": "86db714e-067b-4756-c4cd-03ba5b5a14e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "#env = gym.make(\"FrozenLake8x8-v0\")\n",
        "env = gym.make(\"FrozenLake-v0\")\n",
        "env.render()\n",
        "print('Number of states: {}'.format(env.observation_space.n))\n",
        "print(\"number of actions: {}\".format(env.action_space.n))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Number of states: 16\n",
            "number of actions: 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZDtaAhPPbGB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make some changes for git\n",
        "class Q_learn:\n",
        "  def __init__(self, env, optimizer, episodes, explore):\n",
        "    self._state_size = env.observation_space.n\n",
        "    self._action_size = env.action_space.n\n",
        "    self._optimizer = optimizer\n",
        "    self.gamma = 0.9\n",
        "    self.episodes = episodes\n",
        "    self.epsilon = explore\n",
        "    #We save the experience for sucess and failures induvidually. \n",
        "    #Usually there are two many faliure and we get rewards only when we readch the goal. There are no intermediate rewards.\n",
        "    # Consequently, distribution over sucess /failures are imbalanced.\n",
        "    #So training the algo on failures is useful, only if we have some date where we succeeded.\n",
        "    self.experience_replay_s = deque(maxlen = 2000) \n",
        "    self.experience_replay_f = deque(maxlen = 2000)\n",
        "\n",
        "    self.q_network = self.network_model()\n",
        "    #self.target_network = self.network_model()\n",
        "    #self.copy_weights()\n",
        "\n",
        "  def store(self, state, action, reward, next_state, terminated):\n",
        "    \"\"\"\n",
        "    We save the experience for sucess and failures induvidually. \n",
        "    \"\"\"\n",
        "    if reward>0.0:\n",
        "      self.experience_replay_s.append((state, action, reward, next_state, terminated))\n",
        "    else:\n",
        "      self.experience_replay_f.append((state, action, reward, next_state, terminated))\n",
        "\n",
        "  def eps_policy(self, state):\n",
        "    \"\"\"\n",
        "    Episilon greedy policy\n",
        "    \"\"\"\n",
        "    if np.random.rand() <= self.epsilon:\n",
        "      return env.action_space.sample()\n",
        "    else:\n",
        "      q_values = self.q_network.predict(np.array([state]))\n",
        "      return np.argmax(q_values[0])\n",
        "\n",
        "  def network_model(self):\n",
        "    \"\"\"\n",
        "    NN model for Q value function\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(Dense(50, activation='relu', input_shape=[1]))\n",
        "    model.add(Dense(50, activation='relu'))\n",
        "    model.add(Dense(self._action_size, activation='relu'))\n",
        "    model.compile(loss = 'mse', optimizer = self._optimizer)\n",
        "    return model\n",
        "  \n",
        "\n",
        "  def test_fun(self, state, action, reward, next_state, terminated):\n",
        "    target = self.q_network.predict(np.array([state]))\n",
        "    if terminated:\n",
        "      target[0][int(action)] = reward\n",
        "    else:\n",
        "      A = np.ones(self._action_size, dtype=float)*self.epsilon/self._action_size # [eps/n ....n.... eps/n] \n",
        "      q_values = self.q_network.predict(np.array([state]))\n",
        "      best_action = np.argmax(q_values[0])\n",
        "      A[best_action] += (1.0 - self.epsilon)\n",
        "      new_action = np.random.choice(self._action_size, p = A)\n",
        "      q_val_next_state = self.q_network.predict(np.array([next_state]))\n",
        "      Expected_next_q_value = np.sum(np.array(q_val_next_state)*A)\n",
        "      target[0][int(action)] = reward + self.gamma*Expected_next_q_value\n",
        "    return list(target[0])\n",
        "\n",
        "  def train(self, batch_size):\n",
        "    minibatch_s = np.array(random.choices(self.experience_replay_s, k = batch_size))\n",
        "    minibatch_f = np.array(random.choices(self.experience_replay_f, k = batch_size))\n",
        "    minibatch = np.concatenate((minibatch_s,minibatch_f), axis =0)\n",
        "    batch_state = minibatch[:,0]\n",
        "    batch_target = np.array([self.test_fun(state, action, reward, next_state, terminated) for state, action, reward, next_state, terminated in minibatch])\n",
        "      \n",
        "    self.q_network.fit(batch_state, batch_target, epochs = 3, verbose = 1)\n",
        "    \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "de6HLTCFdvSQ",
        "colab_type": "code",
        "outputId": "e43784bc-26a4-465e-93a0-ee50ac020720",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "optimizer = Adam(learning_rate=0.01)\n",
        "test_agent = Q_learn(env, optimizer, episodes= 100, explore =0.9)\n",
        " \n",
        "print(\"Testing the model:\")\n",
        "test_agent.q_network.build()\n",
        "print(test_agent.q_network.summary())\n",
        "test_predicted = test_agent.q_network.predict(np.array([env.observation_space.sample()]))\n",
        "print(\"predict the q_values for a random sample is {}\".format(test_predicted))\n",
        "test_history = test_agent.q_network.fit(np.array([env.observation_space.sample()]), test_predicted, epochs=1)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing the model:\n",
            "Model: \"sequential_19\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_57 (Dense)             (None, 50)                100       \n",
            "_________________________________________________________________\n",
            "dense_58 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_59 (Dense)             (None, 4)                 204       \n",
            "=================================================================\n",
            "Total params: 2,854\n",
            "Trainable params: 2,854\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "predict the q_values for a random sample is [[0.         0.89512193 0.         0.        ]]\n",
            "Train on 1 samples\n",
            "1/1 [==============================] - 0s 283ms/sample - loss: 0.0056\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egS-bNYA6DFU",
        "colab_type": "code",
        "outputId": "be3cb22c-4823-410e-9982-b61ccc6bcdf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "optimizer = Adam(learning_rate=0.01)\n",
        "\n",
        "agent = Q_learn(env, optimizer,episodes=10000, explore = 0.9)\n",
        "\n",
        "batch_size = 500\n",
        "num_of_episodes = 10000\n",
        "timesteps_per_episode = 128\n",
        "Fail = 0\n",
        "S = 0\n",
        "for epi in range(0,num_of_episodes):\n",
        "  state = env.reset()\n",
        "  #state = np.array([state])\n",
        "\n",
        "  reward = 0\n",
        "  if S>=50:\n",
        "    agent.epsilon = 0.1\n",
        "  else:\n",
        "    agent.epsilon = 0.05\n",
        "\n",
        "  terminated  = False\n",
        "  #bar = progressbar.ProgressBar(maxval=timesteps_per_episode/10, widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
        "  #bar.start()\n",
        "\n",
        "  for steps in range(timesteps_per_episode):\n",
        "    action = agent.eps_policy(state)\n",
        "    #print(action)\n",
        "    next_state, reward, terminated, _  = env.step(action)\n",
        "    agent.store(state, action, reward, next_state, terminated)\n",
        "    state = next_state\n",
        "    #print(agent.experience_replay)\n",
        "\n",
        "    \n",
        "    #bar.finish()\n",
        "    if (epi)%100 == 0:\n",
        "      if S>50:\n",
        "        print(\"**********************************\")\n",
        "        env.render()\n",
        "        print(\"**********************************\")\n",
        "    if terminated:\n",
        "      if reward > 0.0:\n",
        "        S=S+1\n",
        "      else:\n",
        "        Fail = Fail +1\n",
        "      break\n",
        "  if S>=50:\n",
        "    agent.train(batch_size)\n",
        "    if epi%100 ==0:\n",
        "      print(\"Episode: {}, S = {}, Fail = {}, exploration ={}\".format(epi + 1, S, Fail, agent.epsilon))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1000 samples\n",
            "Epoch 1/3\n",
            "1000/1000 [==============================] - 0s 366us/sample - loss: 1.0132\n",
            "Epoch 2/3\n",
            "1000/1000 [==============================] - 0s 65us/sample - loss: 0.8251\n",
            "Epoch 3/3\n",
            "1000/1000 [==============================] - 0s 62us/sample - loss: 0.8237\n",
            "Train on 1000 samples\n",
            "Epoch 1/3\n",
            "1000/1000 [==============================] - 0s 72us/sample - loss: 0.5044\n",
            "Epoch 2/3\n",
            "1000/1000 [==============================] - 0s 59us/sample - loss: 0.5056\n",
            "Epoch 3/3\n",
            "1000/1000 [==============================] - 0s 61us/sample - loss: 0.5044\n",
            "Train on 1000 samples\n",
            "Epoch 1/3\n",
            "1000/1000 [==============================] - 0s 64us/sample - loss: 0.4798\n",
            "Epoch 2/3\n",
            "1000/1000 [==============================] - 0s 65us/sample - loss: 0.4768\n",
            "Epoch 3/3\n",
            "1000/1000 [==============================] - 0s 58us/sample - loss: 0.4777\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQSvJ98qI6my",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}